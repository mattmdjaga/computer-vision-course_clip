# Contrastive Learning

Before diving into the diffrent losses used to train models like CLIP, it is important to have a clear understanding about what contrastive learning is. Contrastive Learning is an unsupervised deep learning method aimed at representation learning. Its objective is to develop a data representation where similar items are positioned closely in the representation space, and dissimilar items are distinctly separated.

In image below, we have an example where we want to keep the representation from dogs closer to other dogs, but also far from cats.
![image info](./contrastive_learning.png)

# Losses

## Contrastive loss

Contrastive loss is one of the first training objectives that was used for contrastive learning. It takes as input a PAIR of samples that can be similar or dissimilar, and the objective is to bring similar samples closes and dissimilar distant.

Technically speaking, imagine that we have a list of input samples $\{x_n\}$ from multiple classes. What we want is a function where examples from the same class have their embeddings close in the embedding space, and also examples from different classes are far apart. Translating this to a mathematical equation, what we have is:

$$L = \mathbb{1}[y_i = y_j]||x_i - x_j||^2 + \mathbb{1}[y_i \neq y_j]max(0, \epsilon - ||x_i - x_j||^2)$$

Explaining in simple terms:

- If the samples are similar ($y_i = y_j$), then we minimize the term $||x_i - x_j||^2$ that corresponds to their Euclidean distance, i.e., we want to make them closer;
- If the samples are dissimilar ($y_i \neq y_j$), then we minimize the term $max(0, \epsilon - ||x_i - x_j||^2)$ that is equivalent to maximizing their euclidean distance until some limit $\epsilon$, i.e., we want to make them distant from each other.


# References

- https://lilianweng.github.io/posts/2021-05-31-contrastive/
- https://www.baeldung.com/cs/contrastive-learning